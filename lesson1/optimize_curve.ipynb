{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создайте случайный FloatTensor размера 3x4x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.FloatTensor(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.6845e+24,  1.8611e+34,  4.2740e-43,  0.0000e+00, -1.7039e+27],\n",
       "         [ 4.5580e-41, -1.7039e+27,  4.5580e-41,  1.8475e+20,  1.3585e-19],\n",
       "         [ 4.0274e-11,  1.9146e-19,  2.3301e-09,  2.5491e+21,  1.3563e-19],\n",
       "         [ 8.4828e-33,  1.3563e-19,  9.4500e+08,  7.9309e+34,  1.5788e-19]],\n",
       "\n",
       "        [[ 2.7604e+20,  8.6769e-33,  1.6114e-19,  1.6020e-19,  7.4901e+11],\n",
       "         [ 6.8901e+22,  3.1892e-09,  2.9479e+29,  1.7442e+28,  2.5171e-12],\n",
       "         [ 5.4667e-11,  1.0736e+18,  1.8037e+28,  6.0644e-02,  3.1246e-15],\n",
       "         [ 7.7073e+20,  2.2228e-15,  1.4582e-19,  7.5388e+17,  1.8559e-01]],\n",
       "\n",
       "        [[ 1.0888e-32,  1.3563e-19,  6.3371e-10,  1.3585e-19,  2.3765e+20],\n",
       "         [ 1.9316e-19,  2.3301e-09,  4.0786e+22,  1.3563e-19,  9.9368e-33],\n",
       "         [ 3.0881e+29,  1.8916e+23,  7.1443e+31,  1.3496e-01,  1.8475e+20],\n",
       "         [ 7.3162e+28,  1.7261e+25,  3.2139e-12,  1.1286e+27,  2.8405e+20]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выведите его форму (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приведите его к форме 6 X 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x6x10=torch.reshape(x, (6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.6845e+24,  1.8611e+34,  4.2740e-43,  0.0000e+00, -1.7039e+27],\n",
       "         [ 4.5580e-41, -1.7039e+27,  4.5580e-41,  1.8475e+20,  1.3585e-19],\n",
       "         [ 4.0274e-11,  1.9146e-19,  2.3301e-09,  2.5491e+21,  1.3563e-19],\n",
       "         [ 8.4828e-33,  1.3563e-19,  9.4500e+08,  7.9309e+34,  1.5788e-19]],\n",
       "\n",
       "        [[ 2.7604e+20,  8.6769e-33,  1.6114e-19,  1.6020e-19,  7.4901e+11],\n",
       "         [ 6.8901e+22,  3.1892e-09,  2.9479e+29,  1.7442e+28,  2.5171e-12],\n",
       "         [ 5.4667e-11,  1.0736e+18,  1.8037e+28,  6.0644e-02,  3.1246e-15],\n",
       "         [ 7.7073e+20,  2.2228e-15,  1.4582e-19,  7.5388e+17,  1.8559e-01]],\n",
       "\n",
       "        [[ 1.0888e-32,  1.3563e-19,  6.3371e-10,  1.3585e-19,  2.3765e+20],\n",
       "         [ 1.9316e-19,  2.3301e-09,  4.0786e+22,  1.3563e-19,  9.9368e-33],\n",
       "         [ 3.0881e+29,  1.8916e+23,  7.1443e+31,  1.3496e-01,  1.8475e+20],\n",
       "         [ 7.3162e+28,  1.7261e+25,  3.2139e-12,  1.1286e+27,  2.8405e+20]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6x10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Умножьте его на вектор [1, 4, 2, 2, 1] поэлементно\n",
    "\n",
    "(Здесь я могу ошибаться, но я взял исходный вектор, так как он совпадает по размерности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.6845e+24,  7.4442e+34,  8.5479e-43,  0.0000e+00, -1.7039e+27],\n",
       "         [ 4.5580e-41, -6.8154e+27,  9.1160e-41,  3.6950e+20,  1.3585e-19],\n",
       "         [ 4.0274e-11,  7.6585e-19,  4.6603e-09,  5.0982e+21,  1.3563e-19],\n",
       "         [ 8.4828e-33,  5.4253e-19,  1.8900e+09,  1.5862e+35,  1.5788e-19]],\n",
       "\n",
       "        [[ 2.7604e+20,  3.4708e-32,  3.2228e-19,  3.2039e-19,  7.4901e+11],\n",
       "         [ 6.8901e+22,  1.2757e-08,  5.8957e+29,  3.4884e+28,  2.5171e-12],\n",
       "         [ 5.4667e-11,  4.2943e+18,  3.6074e+28,  1.2129e-01,  3.1246e-15],\n",
       "         [ 7.7073e+20,  8.8914e-15,  2.9164e-19,  1.5078e+18,  1.8559e-01]],\n",
       "\n",
       "        [[ 1.0888e-32,  5.4253e-19,  1.2674e-09,  2.7170e-19,  2.3765e+20],\n",
       "         [ 1.9316e-19,  9.3205e-09,  8.1572e+22,  2.7126e-19,  9.9368e-33],\n",
       "         [ 3.0881e+29,  7.5666e+23,  1.4289e+32,  2.6993e-01,  1.8475e+20],\n",
       "         [ 7.3162e+28,  6.9044e+25,  6.4278e-12,  2.2573e+27,  2.8405e+20]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*torch.LongTensor([1, 4, 2, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Умножьте тензор матрично на себя, чтобы результат был размерности 6x6\n",
    "\n",
    "(А здесь снова взял вектор размерности 6х10, полученный ранее)\n",
    "\n",
    "(x6x10 - это название переменной)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6845e+24,  1.8611e+34,  4.2740e-43,  0.0000e+00, -1.7039e+27,\n",
       "          4.5580e-41, -1.7039e+27,  4.5580e-41,  1.8475e+20,  1.3585e-19],\n",
       "        [ 4.0274e-11,  1.9146e-19,  2.3301e-09,  2.5491e+21,  1.3563e-19,\n",
       "          8.4828e-33,  1.3563e-19,  9.4500e+08,  7.9309e+34,  1.5788e-19],\n",
       "        [ 2.7604e+20,  8.6769e-33,  1.6114e-19,  1.6020e-19,  7.4901e+11,\n",
       "          6.8901e+22,  3.1892e-09,  2.9479e+29,  1.7442e+28,  2.5171e-12],\n",
       "        [ 5.4667e-11,  1.0736e+18,  1.8037e+28,  6.0644e-02,  3.1246e-15,\n",
       "          7.7073e+20,  2.2228e-15,  1.4582e-19,  7.5388e+17,  1.8559e-01],\n",
       "        [ 1.0888e-32,  1.3563e-19,  6.3371e-10,  1.3585e-19,  2.3765e+20,\n",
       "          1.9316e-19,  2.3301e-09,  4.0786e+22,  1.3563e-19,  9.9368e-33],\n",
       "        [ 3.0881e+29,  1.8916e+23,  7.1443e+31,  1.3496e-01,  1.8475e+20,\n",
       "          7.3162e+28,  1.7261e+25,  3.2139e-12,  1.1286e+27,  2.8405e+20]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.matmul(x6x10,torch.transpose(x6x10,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       inf,        inf,        nan,        inf,       -inf,        nan],\n",
       "        [       inf,        inf,        inf,        inf, 3.8543e+31,        inf],\n",
       "        [       nan,        inf,        inf,        inf,        inf,        inf],\n",
       "        [       inf,        inf,        inf,        inf, 1.1430e+19,        inf],\n",
       "        [      -inf, 3.8543e+31,        inf, 1.1430e+19,        inf,        inf],\n",
       "        [       nan,        inf,        inf,        inf,        inf,        inf]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитайте производную функции y = x**3 + z - 75t в точке (1, 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = torch.DoubleTensor([1., 0.5, 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь два варианта:\n",
    "1. с тремя независимыми переменными (x, z, t)\n",
    "2. с вектором переменных, каждая из координат которого, по сути, представляет независимую переменную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 1. С независимыми переменными x, z, t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Variable(torch.DoubleTensor([1.]), requires_grad=True)\n",
    "z=Variable(torch.DoubleTensor([0.5]), requires_grad=True)\n",
    "t=Variable(torch.DoubleTensor([2.]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pow(x,3)+z-75*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-148.5000], dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производная по каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], dtype=torch.float64)\n",
      "tensor([1.], dtype=torch.float64)\n",
      "tensor([-75.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(z.grad)\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 2. Переменная-вектор p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Variable(point, requires_grad=True)\n",
    "o = pow(p[0], 3)+p[1]-75*p[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.,   1., -75.], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих вариантах производные по направлениям совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создайте единичный тензор размера 5x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переведите его в формат numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_numpy = t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте теперь пооптимизируем: возьмите функцию y = x**w1 - 2 * x**2 + 5\n",
    "### Посчитайте "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Variable(torch.rand(3), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение класса - не очень удачная попытка сделать функцию-класс, которая при вызове pred = f(x, w1) внутри обновляла бы значение градиента.\n",
    "Но получилось более громоздко."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_t:\n",
    "    #w1 = Variable(torch.rand(3), requires_grad=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f(x, w1):\n",
    "        return x**w1 - 2*x**2 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.rand(3), requires_grad = True)\n",
    "y = Variable(torch.rand(3), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  tensor([0.4486, 0.5936, 0.5186], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"w1: \", w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6800, 0.2177, 0.1886], requires_grad=True),\n",
       " tensor([0.5310, 0.2276, 0.3741]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  24.16851234436035\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "pred = custom_t.f(x, w1)\n",
    "loss = criterion(pred, y)\n",
    "print(\"loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(w1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8828, -1.6401, -1.2886])\n"
     ]
    }
   ],
   "source": [
    "print(w1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение функции, по сути, количества эпох оптимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_loop(n=100):\n",
    "    for i in range(1,n):\n",
    "        optimizer.zero_grad()\n",
    "        pred = custom_t.f(x, w1)\n",
    "        loss = criterion(pred, y)\n",
    "        print(\"loss: \", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование функции с нужным количеством циклов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  18.380823135375977\n",
      "loss:  18.38068962097168\n",
      "loss:  18.38055419921875\n",
      "loss:  18.380420684814453\n",
      "loss:  18.38028907775879\n",
      "loss:  18.380151748657227\n",
      "loss:  18.380020141601562\n",
      "loss:  18.3798885345459\n",
      "loss:  18.3797550201416\n",
      "loss:  18.379621505737305\n",
      "loss:  18.379487991333008\n",
      "loss:  18.37935447692871\n",
      "loss:  18.37922477722168\n",
      "loss:  18.37908935546875\n",
      "loss:  18.37895965576172\n",
      "loss:  18.37882423400879\n",
      "loss:  18.378692626953125\n",
      "loss:  18.37856101989746\n",
      "loss:  18.378427505493164\n",
      "loss:  18.378297805786133\n",
      "loss:  18.378164291381836\n",
      "loss:  18.378034591674805\n",
      "loss:  18.377901077270508\n",
      "loss:  18.377771377563477\n",
      "loss:  18.377639770507812\n",
      "loss:  18.37750816345215\n",
      "loss:  18.377378463745117\n",
      "loss:  18.37724494934082\n",
      "loss:  18.377113342285156\n",
      "loss:  18.376983642578125\n",
      "loss:  18.376853942871094\n",
      "loss:  18.376724243164062\n",
      "loss:  18.37659454345703\n",
      "loss:  18.376462936401367\n",
      "loss:  18.376331329345703\n",
      "loss:  18.376203536987305\n",
      "loss:  18.376073837280273\n",
      "loss:  18.375944137573242\n",
      "loss:  18.37581443786621\n",
      "loss:  18.37568473815918\n",
      "loss:  18.37555503845215\n",
      "loss:  18.375425338745117\n",
      "loss:  18.37529754638672\n",
      "loss:  18.375167846679688\n",
      "loss:  18.37504005432129\n",
      "loss:  18.374910354614258\n",
      "loss:  18.374780654907227\n",
      "loss:  18.374652862548828\n",
      "loss:  18.37452507019043\n",
      "loss:  18.37439727783203\n",
      "loss:  18.374269485473633\n",
      "loss:  18.3741397857666\n",
      "loss:  18.374011993408203\n",
      "loss:  18.373884201049805\n",
      "loss:  18.37375831604004\n",
      "loss:  18.373628616333008\n",
      "loss:  18.373502731323242\n",
      "loss:  18.373374938964844\n",
      "loss:  18.373247146606445\n",
      "loss:  18.373119354248047\n",
      "loss:  18.37299156188965\n",
      "loss:  18.372867584228516\n",
      "loss:  18.372739791870117\n",
      "loss:  18.37261199951172\n",
      "loss:  18.372488021850586\n",
      "loss:  18.372360229492188\n",
      "loss:  18.37223243713379\n",
      "loss:  18.372106552124023\n",
      "loss:  18.371980667114258\n",
      "loss:  18.371854782104492\n",
      "loss:  18.371728897094727\n",
      "loss:  18.37160301208496\n",
      "loss:  18.371477127075195\n",
      "loss:  18.371353149414062\n",
      "loss:  18.371225357055664\n",
      "loss:  18.37110137939453\n",
      "loss:  18.3709774017334\n",
      "loss:  18.370851516723633\n",
      "loss:  18.370725631713867\n",
      "loss:  18.3705997467041\n",
      "loss:  18.3704776763916\n",
      "loss:  18.370351791381836\n",
      "loss:  18.37022590637207\n",
      "loss:  18.37010383605957\n",
      "loss:  18.369977951049805\n",
      "loss:  18.36985206604004\n",
      "loss:  18.36972999572754\n",
      "loss:  18.369606018066406\n",
      "loss:  18.369483947753906\n",
      "loss:  18.36935806274414\n",
      "loss:  18.36923599243164\n",
      "loss:  18.369112014770508\n",
      "loss:  18.368988037109375\n",
      "loss:  18.368864059448242\n",
      "loss:  18.368741989135742\n",
      "loss:  18.368619918823242\n",
      "loss:  18.368494033813477\n",
      "loss:  18.36837387084961\n",
      "loss:  18.368249893188477\n",
      "loss:  18.368125915527344\n",
      "loss:  18.368003845214844\n",
      "loss:  18.367881774902344\n",
      "loss:  18.367759704589844\n",
      "loss:  18.367637634277344\n",
      "loss:  18.367515563964844\n",
      "loss:  18.367393493652344\n",
      "loss:  18.367273330688477\n",
      "loss:  18.367151260375977\n",
      "loss:  18.367029190063477\n",
      "loss:  18.366905212402344\n",
      "loss:  18.366788864135742\n",
      "loss:  18.366662979125977\n",
      "loss:  18.366544723510742\n",
      "loss:  18.366422653198242\n",
      "loss:  18.366300582885742\n",
      "loss:  18.366180419921875\n",
      "loss:  18.366058349609375\n",
      "loss:  18.365938186645508\n",
      "loss:  18.365816116333008\n",
      "loss:  18.36569595336914\n",
      "loss:  18.365575790405273\n",
      "loss:  18.365455627441406\n",
      "loss:  18.365337371826172\n",
      "loss:  18.365217208862305\n",
      "loss:  18.365095138549805\n",
      "loss:  18.36497688293457\n",
      "loss:  18.364858627319336\n",
      "loss:  18.364736557006836\n",
      "loss:  18.364620208740234\n",
      "loss:  18.364500045776367\n",
      "loss:  18.364377975463867\n",
      "loss:  18.364259719848633\n",
      "loss:  18.3641414642334\n",
      "loss:  18.36402130126953\n",
      "loss:  18.363901138305664\n",
      "loss:  18.363784790039062\n",
      "loss:  18.363664627075195\n",
      "loss:  18.363548278808594\n",
      "loss:  18.363428115844727\n",
      "loss:  18.363309860229492\n",
      "loss:  18.363191604614258\n",
      "loss:  18.363075256347656\n",
      "loss:  18.36295509338379\n",
      "loss:  18.362838745117188\n",
      "loss:  18.36271858215332\n",
      "loss:  18.36260223388672\n",
      "loss:  18.36248207092285\n",
      "loss:  18.36236572265625\n",
      "loss:  18.36224937438965\n",
      "loss:  18.362133026123047\n",
      "loss:  18.362014770507812\n",
      "loss:  18.36189842224121\n",
      "loss:  18.361780166625977\n",
      "loss:  18.361663818359375\n",
      "loss:  18.361547470092773\n",
      "loss:  18.36142921447754\n",
      "loss:  18.361312866210938\n",
      "loss:  18.36119842529297\n",
      "loss:  18.361082077026367\n",
      "loss:  18.360963821411133\n",
      "loss:  18.36084747314453\n",
      "loss:  18.360733032226562\n",
      "loss:  18.360618591308594\n",
      "loss:  18.36050033569336\n",
      "loss:  18.360383987426758\n",
      "loss:  18.36026954650879\n",
      "loss:  18.360151290893555\n",
      "loss:  18.360036849975586\n",
      "loss:  18.359920501708984\n",
      "loss:  18.35980987548828\n",
      "loss:  18.359689712524414\n",
      "loss:  18.35957908630371\n",
      "loss:  18.359458923339844\n",
      "loss:  18.359346389770508\n",
      "loss:  18.35923194885254\n",
      "loss:  18.35911750793457\n",
      "loss:  18.35900115966797\n",
      "loss:  18.358888626098633\n",
      "loss:  18.35877227783203\n",
      "loss:  18.358659744262695\n",
      "loss:  18.358543395996094\n",
      "loss:  18.358430862426758\n",
      "loss:  18.35831642150879\n",
      "loss:  18.35820198059082\n",
      "loss:  18.358089447021484\n",
      "loss:  18.357975006103516\n",
      "loss:  18.357860565185547\n",
      "loss:  18.35774803161621\n",
      "loss:  18.357635498046875\n",
      "loss:  18.357521057128906\n",
      "loss:  18.35740852355957\n",
      "loss:  18.3572940826416\n",
      "loss:  18.357179641723633\n",
      "loss:  18.357070922851562\n",
      "loss:  18.356956481933594\n",
      "loss:  18.356843948364258\n",
      "loss:  18.35672950744629\n",
      "loss:  18.356618881225586\n",
      "loss:  18.356504440307617\n",
      "loss:  18.356393814086914\n",
      "loss:  18.356281280517578\n",
      "loss:  18.356168746948242\n",
      "loss:  18.35605812072754\n",
      "loss:  18.35594367980957\n",
      "loss:  18.355833053588867\n",
      "loss:  18.35572052001953\n",
      "loss:  18.355609893798828\n",
      "loss:  18.355497360229492\n",
      "loss:  18.355384826660156\n",
      "loss:  18.355276107788086\n",
      "loss:  18.355161666870117\n",
      "loss:  18.355051040649414\n",
      "loss:  18.35494041442871\n",
      "loss:  18.354829788208008\n",
      "loss:  18.354719161987305\n",
      "loss:  18.35460662841797\n",
      "loss:  18.3544979095459\n",
      "loss:  18.354387283325195\n",
      "loss:  18.354276657104492\n",
      "loss:  18.354164123535156\n",
      "loss:  18.35405731201172\n",
      "loss:  18.353944778442383\n",
      "loss:  18.35383415222168\n",
      "loss:  18.353723526000977\n",
      "loss:  18.353612899780273\n",
      "loss:  18.353504180908203\n",
      "loss:  18.353395462036133\n",
      "loss:  18.35328483581543\n",
      "loss:  18.353174209594727\n",
      "loss:  18.353065490722656\n",
      "loss:  18.352956771850586\n",
      "loss:  18.352846145629883\n",
      "loss:  18.352737426757812\n",
      "loss:  18.352628707885742\n",
      "loss:  18.35251808166504\n",
      "loss:  18.35240936279297\n",
      "loss:  18.35230255126953\n",
      "loss:  18.35219383239746\n",
      "loss:  18.352083206176758\n",
      "loss:  18.351974487304688\n",
      "loss:  18.35186767578125\n",
      "loss:  18.35175895690918\n",
      "loss:  18.351648330688477\n",
      "loss:  18.35154151916504\n",
      "loss:  18.351430892944336\n",
      "loss:  18.3513240814209\n",
      "loss:  18.35121726989746\n",
      "loss:  18.35110855102539\n",
      "loss:  18.351001739501953\n",
      "loss:  18.350893020629883\n",
      "loss:  18.350786209106445\n",
      "loss:  18.350679397583008\n",
      "loss:  18.35057258605957\n",
      "loss:  18.3504638671875\n",
      "loss:  18.350357055664062\n",
      "loss:  18.350250244140625\n",
      "loss:  18.350141525268555\n",
      "loss:  18.350034713745117\n",
      "loss:  18.34992790222168\n",
      "loss:  18.349821090698242\n",
      "loss:  18.349714279174805\n",
      "loss:  18.349607467651367\n",
      "loss:  18.34950065612793\n",
      "loss:  18.349393844604492\n",
      "loss:  18.349287033081055\n",
      "loss:  18.34918212890625\n",
      "loss:  18.349075317382812\n",
      "loss:  18.348970413208008\n",
      "loss:  18.34886360168457\n",
      "loss:  18.348756790161133\n",
      "loss:  18.348649978637695\n",
      "loss:  18.348546981811523\n",
      "loss:  18.348440170288086\n",
      "loss:  18.34833335876465\n",
      "loss:  18.348230361938477\n",
      "loss:  18.34812355041504\n",
      "loss:  18.348018646240234\n",
      "loss:  18.34791374206543\n",
      "loss:  18.347808837890625\n",
      "loss:  18.34770393371582\n",
      "loss:  18.347597122192383\n",
      "loss:  18.34749412536621\n",
      "loss:  18.347389221191406\n",
      "loss:  18.347280502319336\n",
      "loss:  18.347177505493164\n",
      "loss:  18.347074508666992\n",
      "loss:  18.346969604492188\n",
      "loss:  18.346864700317383\n",
      "loss:  18.34676170349121\n",
      "loss:  18.346654891967773\n",
      "loss:  18.3465518951416\n",
      "loss:  18.34644889831543\n",
      "loss:  18.346345901489258\n",
      "loss:  18.346240997314453\n",
      "loss:  18.34613800048828\n",
      "loss:  18.346033096313477\n",
      "loss:  18.345930099487305\n",
      "loss:  18.345827102661133\n",
      "loss:  18.34572410583496\n",
      "loss:  18.345617294311523\n",
      "loss:  18.34551429748535\n",
      "loss:  18.345413208007812\n",
      "loss:  18.345308303833008\n",
      "loss:  18.345205307006836\n",
      "loss:  18.345102310180664\n",
      "loss:  18.344999313354492\n",
      "loss:  18.344898223876953\n",
      "loss:  18.34479522705078\n",
      "loss:  18.344690322875977\n",
      "loss:  18.34459114074707\n",
      "loss:  18.3444881439209\n",
      "loss:  18.344385147094727\n",
      "loss:  18.344282150268555\n",
      "loss:  18.34418296813965\n",
      "loss:  18.344078063964844\n",
      "loss:  18.343975067138672\n",
      "loss:  18.3438777923584\n",
      "loss:  18.343772888183594\n",
      "loss:  18.343669891357422\n",
      "loss:  18.343568801879883\n",
      "loss:  18.343469619750977\n",
      "loss:  18.343364715576172\n",
      "loss:  18.343263626098633\n",
      "loss:  18.343164443969727\n",
      "loss:  18.343063354492188\n",
      "loss:  18.342958450317383\n",
      "loss:  18.34286117553711\n",
      "loss:  18.342758178710938\n",
      "loss:  18.3426570892334\n",
      "loss:  18.34255599975586\n",
      "loss:  18.342458724975586\n",
      "loss:  18.342355728149414\n",
      "loss:  18.342254638671875\n",
      "loss:  18.342153549194336\n",
      "loss:  18.34205436706543\n",
      "loss:  18.341951370239258\n",
      "loss:  18.34185218811035\n",
      "loss:  18.341753005981445\n",
      "loss:  18.341651916503906\n",
      "loss:  18.341552734375\n",
      "loss:  18.34145164489746\n",
      "loss:  18.341354370117188\n",
      "loss:  18.34125328063965\n",
      "loss:  18.341150283813477\n",
      "loss:  18.34105110168457\n",
      "loss:  18.34095573425293\n",
      "loss:  18.340856552124023\n",
      "loss:  18.34075355529785\n",
      "loss:  18.340654373168945\n",
      "loss:  18.34055519104004\n",
      "loss:  18.340457916259766\n",
      "loss:  18.340360641479492\n",
      "loss:  18.34025764465332\n",
      "loss:  18.340158462524414\n",
      "loss:  18.340059280395508\n",
      "loss:  18.339962005615234\n",
      "loss:  18.33986473083496\n",
      "loss:  18.339765548706055\n",
      "loss:  18.33966636657715\n",
      "loss:  18.339569091796875\n",
      "loss:  18.33946990966797\n",
      "loss:  18.339372634887695\n",
      "loss:  18.33927345275879\n",
      "loss:  18.339176177978516\n",
      "loss:  18.339078903198242\n",
      "loss:  18.338979721069336\n",
      "loss:  18.338882446289062\n",
      "loss:  18.338783264160156\n",
      "loss:  18.338685989379883\n",
      "loss:  18.338586807250977\n",
      "loss:  18.338491439819336\n",
      "loss:  18.33839225769043\n",
      "loss:  18.338293075561523\n",
      "loss:  18.33819580078125\n",
      "loss:  18.338102340698242\n",
      "loss:  18.338003158569336\n",
      "loss:  18.337905883789062\n",
      "loss:  18.33780860900879\n",
      "loss:  18.33771324157715\n",
      "loss:  18.337615966796875\n",
      "loss:  18.3375186920166\n",
      "loss:  18.337419509887695\n",
      "loss:  18.337324142456055\n",
      "loss:  18.337228775024414\n",
      "loss:  18.33713150024414\n",
      "loss:  18.337034225463867\n",
      "loss:  18.336938858032227\n",
      "loss:  18.336843490600586\n",
      "loss:  18.336746215820312\n",
      "loss:  18.33664894104004\n",
      "loss:  18.3365535736084\n",
      "loss:  18.336458206176758\n",
      "loss:  18.336362838745117\n",
      "loss:  18.33626365661621\n",
      "loss:  18.33616828918457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  18.33607292175293\n",
      "loss:  18.335981369018555\n",
      "loss:  18.33588218688965\n",
      "loss:  18.335786819458008\n",
      "loss:  18.335693359375\n",
      "loss:  18.335596084594727\n",
      "loss:  18.335500717163086\n",
      "loss:  18.335405349731445\n",
      "loss:  18.335309982299805\n",
      "loss:  18.33521842956543\n",
      "loss:  18.33512306213379\n",
      "loss:  18.33502769470215\n",
      "loss:  18.334932327270508\n",
      "loss:  18.334836959838867\n",
      "loss:  18.334741592407227\n",
      "loss:  18.33464813232422\n",
      "loss:  18.33455467224121\n",
      "loss:  18.33445930480957\n",
      "loss:  18.33436393737793\n",
      "loss:  18.334272384643555\n",
      "loss:  18.33417320251465\n",
      "loss:  18.334081649780273\n",
      "loss:  18.333988189697266\n",
      "loss:  18.333892822265625\n",
      "loss:  18.333799362182617\n",
      "loss:  18.333703994750977\n",
      "loss:  18.3336124420166\n",
      "loss:  18.333518981933594\n",
      "loss:  18.333425521850586\n",
      "loss:  18.333330154418945\n",
      "loss:  18.33323860168457\n",
      "loss:  18.333145141601562\n",
      "loss:  18.333051681518555\n",
      "loss:  18.33296012878418\n",
      "loss:  18.332862854003906\n",
      "loss:  18.33277130126953\n",
      "loss:  18.332677841186523\n",
      "loss:  18.33258628845215\n",
      "loss:  18.33249282836914\n",
      "loss:  18.332399368286133\n",
      "loss:  18.332307815551758\n",
      "loss:  18.33221435546875\n",
      "loss:  18.332122802734375\n",
      "loss:  18.33203125\n",
      "loss:  18.331937789916992\n",
      "loss:  18.33184242248535\n",
      "loss:  18.331750869750977\n",
      "loss:  18.3316593170166\n",
      "loss:  18.331567764282227\n",
      "loss:  18.33147430419922\n",
      "loss:  18.331384658813477\n",
      "loss:  18.33129119873047\n",
      "loss:  18.331199645996094\n",
      "loss:  18.33110809326172\n",
      "loss:  18.33101463317871\n",
      "loss:  18.33092498779297\n",
      "loss:  18.330833435058594\n",
      "loss:  18.33074188232422\n",
      "loss:  18.330650329589844\n",
      "loss:  18.330556869506836\n",
      "loss:  18.330467224121094\n",
      "loss:  18.33037757873535\n",
      "loss:  18.330284118652344\n",
      "loss:  18.3301944732666\n",
      "loss:  18.330102920532227\n",
      "loss:  18.33001136779785\n",
      "loss:  18.329919815063477\n",
      "loss:  18.3298282623291\n",
      "loss:  18.329740524291992\n",
      "loss:  18.329648971557617\n",
      "loss:  18.329557418823242\n",
      "loss:  18.329465866088867\n",
      "loss:  18.329376220703125\n",
      "loss:  18.32928466796875\n",
      "loss:  18.32919692993164\n",
      "loss:  18.329103469848633\n",
      "loss:  18.329015731811523\n",
      "loss:  18.32892608642578\n",
      "loss:  18.328834533691406\n",
      "loss:  18.328744888305664\n",
      "loss:  18.328657150268555\n",
      "loss:  18.328563690185547\n",
      "loss:  18.328475952148438\n",
      "loss:  18.328384399414062\n",
      "loss:  18.32829475402832\n",
      "loss:  18.32820701599121\n",
      "loss:  18.32811737060547\n",
      "loss:  18.328025817871094\n",
      "loss:  18.32793617248535\n",
      "loss:  18.327848434448242\n",
      "loss:  18.327760696411133\n",
      "loss:  18.327669143676758\n",
      "loss:  18.32758140563965\n",
      "loss:  18.32749366760254\n",
      "loss:  18.327402114868164\n",
      "loss:  18.327312469482422\n",
      "loss:  18.327224731445312\n",
      "loss:  18.327136993408203\n",
      "loss:  18.32704734802246\n",
      "loss:  18.32695960998535\n",
      "loss:  18.326871871948242\n",
      "loss:  18.3267822265625\n",
      "loss:  18.32669448852539\n",
      "loss:  18.326601028442383\n",
      "loss:  18.326515197753906\n",
      "loss:  18.32642936706543\n",
      "loss:  18.326339721679688\n",
      "loss:  18.326251983642578\n",
      "loss:  18.32616424560547\n",
      "loss:  18.326078414916992\n",
      "loss:  18.32598876953125\n",
      "loss:  18.325899124145508\n",
      "loss:  18.3258113861084\n",
      "loss:  18.32572364807129\n",
      "loss:  18.32563591003418\n",
      "loss:  18.32554817199707\n",
      "loss:  18.32546043395996\n",
      "loss:  18.325376510620117\n",
      "loss:  18.325286865234375\n",
      "loss:  18.325199127197266\n",
      "loss:  18.32511329650879\n",
      "loss:  18.325027465820312\n",
      "loss:  18.32493782043457\n",
      "loss:  18.32485008239746\n",
      "loss:  18.32476234436035\n",
      "loss:  18.324676513671875\n",
      "loss:  18.3245906829834\n",
      "loss:  18.32450294494629\n",
      "loss:  18.324417114257812\n",
      "loss:  18.324329376220703\n",
      "loss:  18.32424545288086\n",
      "loss:  18.32415771484375\n",
      "loss:  18.32406997680664\n",
      "loss:  18.323984146118164\n",
      "loss:  18.323898315429688\n",
      "loss:  18.32381248474121\n",
      "loss:  18.3237247467041\n",
      "loss:  18.323637008666992\n",
      "loss:  18.32355308532715\n",
      "loss:  18.32346534729004\n",
      "loss:  18.323381423950195\n",
      "loss:  18.323293685913086\n",
      "loss:  18.323209762573242\n",
      "loss:  18.3231258392334\n",
      "loss:  18.32303810119629\n",
      "loss:  18.322952270507812\n",
      "loss:  18.322866439819336\n",
      "loss:  18.322778701782227\n",
      "loss:  18.322694778442383\n",
      "loss:  18.322608947753906\n",
      "loss:  18.322526931762695\n",
      "loss:  18.322439193725586\n",
      "loss:  18.322351455688477\n",
      "loss:  18.3222713470459\n",
      "loss:  18.32218360900879\n",
      "loss:  18.322099685668945\n",
      "loss:  18.322011947631836\n",
      "loss:  18.321928024291992\n",
      "loss:  18.32184600830078\n",
      "loss:  18.321758270263672\n",
      "loss:  18.321674346923828\n",
      "loss:  18.32158851623535\n",
      "loss:  18.32150650024414\n",
      "loss:  18.321420669555664\n",
      "loss:  18.32133674621582\n",
      "loss:  18.321252822875977\n",
      "loss:  18.3211669921875\n",
      "loss:  18.321083068847656\n",
      "loss:  18.320999145507812\n",
      "loss:  18.32091522216797\n",
      "loss:  18.320831298828125\n",
      "loss:  18.32074546813965\n",
      "loss:  18.320663452148438\n",
      "loss:  18.320579528808594\n",
      "loss:  18.32049560546875\n",
      "loss:  18.320411682128906\n",
      "loss:  18.320329666137695\n",
      "loss:  18.320241928100586\n",
      "loss:  18.320161819458008\n",
      "loss:  18.320077896118164\n",
      "loss:  18.319992065429688\n",
      "loss:  18.319910049438477\n",
      "loss:  18.319826126098633\n",
      "loss:  18.319744110107422\n",
      "loss:  18.319660186767578\n",
      "loss:  18.319578170776367\n",
      "loss:  18.319494247436523\n",
      "loss:  18.319412231445312\n",
      "loss:  18.31932830810547\n",
      "loss:  18.319246292114258\n",
      "loss:  18.319162368774414\n",
      "loss:  18.31907844543457\n",
      "loss:  18.318998336791992\n",
      "loss:  18.31891441345215\n",
      "loss:  18.318832397460938\n",
      "loss:  18.318750381469727\n",
      "loss:  18.318668365478516\n",
      "loss:  18.318586349487305\n",
      "loss:  18.31850242614746\n",
      "loss:  18.318418502807617\n",
      "loss:  18.31833839416504\n",
      "loss:  18.318254470825195\n",
      "loss:  18.31817054748535\n",
      "loss:  18.31808853149414\n",
      "loss:  18.318008422851562\n",
      "loss:  18.31792640686035\n",
      "loss:  18.31784439086914\n",
      "loss:  18.31776237487793\n",
      "loss:  18.31768226623535\n",
      "loss:  18.317598342895508\n",
      "loss:  18.31751823425293\n",
      "loss:  18.31743621826172\n",
      "loss:  18.317354202270508\n",
      "loss:  18.317272186279297\n",
      "loss:  18.31719207763672\n",
      "loss:  18.317110061645508\n",
      "loss:  18.317028045654297\n",
      "loss:  18.31694793701172\n",
      "loss:  18.316869735717773\n",
      "loss:  18.31678581237793\n",
      "loss:  18.31670570373535\n",
      "loss:  18.316621780395508\n",
      "loss:  18.316543579101562\n",
      "loss:  18.31646156311035\n",
      "loss:  18.316381454467773\n",
      "loss:  18.316299438476562\n",
      "loss:  18.31621742248535\n",
      "loss:  18.316139221191406\n",
      "loss:  18.316057205200195\n",
      "loss:  18.31597900390625\n",
      "loss:  18.31589698791504\n",
      "loss:  18.31581687927246\n",
      "loss:  18.31573486328125\n",
      "loss:  18.315656661987305\n",
      "loss:  18.315576553344727\n",
      "loss:  18.31549644470215\n",
      "loss:  18.31541633605957\n",
      "loss:  18.315336227416992\n",
      "loss:  18.315256118774414\n",
      "loss:  18.315176010131836\n",
      "loss:  18.315095901489258\n",
      "loss:  18.315017700195312\n",
      "loss:  18.314937591552734\n",
      "loss:  18.314857482910156\n",
      "loss:  18.314775466918945\n",
      "loss:  18.314697265625\n",
      "loss:  18.314619064331055\n",
      "loss:  18.314538955688477\n",
      "loss:  18.3144588470459\n",
      "loss:  18.31437873840332\n",
      "loss:  18.314302444458008\n",
      "loss:  18.31422233581543\n",
      "loss:  18.31414222717285\n",
      "loss:  18.314062118530273\n",
      "loss:  18.313983917236328\n",
      "loss:  18.31390380859375\n",
      "loss:  18.313825607299805\n",
      "loss:  18.313745498657227\n",
      "loss:  18.313669204711914\n",
      "loss:  18.313589096069336\n",
      "loss:  18.313508987426758\n",
      "loss:  18.313430786132812\n",
      "loss:  18.313352584838867\n",
      "loss:  18.313276290893555\n",
      "loss:  18.313194274902344\n",
      "loss:  18.3131160736084\n",
      "loss:  18.313039779663086\n",
      "loss:  18.312959671020508\n",
      "loss:  18.312881469726562\n",
      "loss:  18.31280517578125\n",
      "loss:  18.312725067138672\n",
      "loss:  18.312646865844727\n",
      "loss:  18.31256866455078\n",
      "loss:  18.312490463256836\n",
      "loss:  18.312410354614258\n",
      "loss:  18.312335968017578\n",
      "loss:  18.312257766723633\n",
      "loss:  18.312179565429688\n",
      "loss:  18.312101364135742\n",
      "loss:  18.31202507019043\n",
      "loss:  18.311948776245117\n",
      "loss:  18.31186866760254\n",
      "loss:  18.311792373657227\n",
      "loss:  18.31171417236328\n",
      "loss:  18.311635971069336\n",
      "loss:  18.311559677124023\n",
      "loss:  18.31148338317871\n",
      "loss:  18.311403274536133\n",
      "loss:  18.31132698059082\n",
      "loss:  18.311250686645508\n",
      "loss:  18.311174392700195\n",
      "loss:  18.31109619140625\n",
      "loss:  18.311017990112305\n",
      "loss:  18.31093978881836\n",
      "loss:  18.31086540222168\n",
      "loss:  18.310789108276367\n",
      "loss:  18.310712814331055\n",
      "loss:  18.310632705688477\n",
      "loss:  18.310558319091797\n",
      "loss:  18.31048011779785\n",
      "loss:  18.31040382385254\n",
      "loss:  18.310327529907227\n",
      "loss:  18.310251235961914\n",
      "loss:  18.310176849365234\n",
      "loss:  18.31009864807129\n",
      "loss:  18.310022354125977\n",
      "loss:  18.30994987487793\n",
      "loss:  18.30986976623535\n",
      "loss:  18.30979347229004\n",
      "loss:  18.309717178344727\n",
      "loss:  18.30964469909668\n",
      "loss:  18.3095645904541\n",
      "loss:  18.309490203857422\n",
      "loss:  18.30941390991211\n",
      "loss:  18.30933952331543\n",
      "loss:  18.30926513671875\n",
      "loss:  18.309188842773438\n",
      "loss:  18.309112548828125\n",
      "loss:  18.30903434753418\n",
      "loss:  18.308961868286133\n",
      "loss:  18.308883666992188\n",
      "loss:  18.308809280395508\n",
      "loss:  18.308732986450195\n",
      "loss:  18.308658599853516\n",
      "loss:  18.308584213256836\n",
      "loss:  18.308509826660156\n",
      "loss:  18.308433532714844\n",
      "loss:  18.30835723876953\n",
      "loss:  18.30828285217285\n",
      "loss:  18.308208465576172\n",
      "loss:  18.308134078979492\n",
      "loss:  18.30805778503418\n",
      "loss:  18.3079833984375\n",
      "loss:  18.30790901184082\n",
      "loss:  18.307832717895508\n",
      "loss:  18.307758331298828\n",
      "loss:  18.30768394470215\n",
      "loss:  18.307607650756836\n",
      "loss:  18.30753517150879\n",
      "loss:  18.307458877563477\n",
      "loss:  18.30738639831543\n",
      "loss:  18.307310104370117\n",
      "loss:  18.307235717773438\n",
      "loss:  18.307161331176758\n",
      "loss:  18.30708885192871\n",
      "loss:  18.30701446533203\n",
      "loss:  18.30694007873535\n",
      "loss:  18.306865692138672\n",
      "loss:  18.306791305541992\n",
      "loss:  18.306718826293945\n",
      "loss:  18.3066463470459\n",
      "loss:  18.30657196044922\n",
      "loss:  18.30649757385254\n",
      "loss:  18.30642318725586\n",
      "loss:  18.30634880065918\n",
      "loss:  18.3062744140625\n",
      "loss:  18.30620002746582\n",
      "loss:  18.306127548217773\n",
      "loss:  18.306055068969727\n",
      "loss:  18.305978775024414\n",
      "loss:  18.305908203125\n",
      "loss:  18.30583381652832\n",
      "loss:  18.305761337280273\n",
      "loss:  18.305686950683594\n",
      "loss:  18.30561637878418\n",
      "loss:  18.3055419921875\n",
      "loss:  18.305469512939453\n",
      "loss:  18.305395126342773\n",
      "loss:  18.305320739746094\n",
      "loss:  18.305246353149414\n",
      "loss:  18.30517578125\n",
      "loss:  18.305105209350586\n",
      "loss:  18.305030822753906\n",
      "loss:  18.30495834350586\n",
      "loss:  18.30488395690918\n",
      "loss:  18.304811477661133\n",
      "loss:  18.304738998413086\n",
      "loss:  18.304664611816406\n",
      "loss:  18.304595947265625\n",
      "loss:  18.304523468017578\n",
      "loss:  18.3044490814209\n",
      "loss:  18.30437660217285\n",
      "loss:  18.304306030273438\n",
      "loss:  18.304231643676758\n",
      "loss:  18.304161071777344\n",
      "loss:  18.30409049987793\n",
      "loss:  18.30401611328125\n",
      "loss:  18.30394172668457\n",
      "loss:  18.30387306213379\n",
      "loss:  18.303800582885742\n",
      "loss:  18.303728103637695\n",
      "loss:  18.30365753173828\n",
      "loss:  18.3035831451416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  18.303512573242188\n",
      "loss:  18.303442001342773\n",
      "loss:  18.303369522094727\n",
      "loss:  18.30329704284668\n",
      "loss:  18.303224563598633\n",
      "loss:  18.30315399169922\n",
      "loss:  18.303083419799805\n",
      "loss:  18.303010940551758\n",
      "loss:  18.302940368652344\n",
      "loss:  18.30286979675293\n",
      "loss:  18.302797317504883\n",
      "loss:  18.302724838256836\n",
      "loss:  18.302656173706055\n",
      "loss:  18.302583694458008\n",
      "loss:  18.30251121520996\n",
      "loss:  18.302438735961914\n",
      "loss:  18.302370071411133\n",
      "loss:  18.30229949951172\n",
      "loss:  18.302228927612305\n",
      "loss:  18.302156448364258\n",
      "loss:  18.302085876464844\n",
      "loss:  18.30201530456543\n",
      "loss:  18.301944732666016\n",
      "loss:  18.301876068115234\n",
      "loss:  18.30180549621582\n",
      "loss:  18.301734924316406\n",
      "loss:  18.301664352416992\n",
      "loss:  18.301591873168945\n",
      "loss:  18.30152130126953\n",
      "loss:  18.301450729370117\n",
      "loss:  18.301382064819336\n",
      "loss:  18.30130958557129\n",
      "loss:  18.301240921020508\n",
      "loss:  18.301170349121094\n",
      "loss:  18.30109977722168\n",
      "loss:  18.3010311126709\n",
      "loss:  18.30095863342285\n",
      "loss:  18.30088996887207\n",
      "loss:  18.300819396972656\n",
      "loss:  18.300750732421875\n",
      "loss:  18.300682067871094\n",
      "loss:  18.30061149597168\n",
      "loss:  18.300540924072266\n",
      "loss:  18.30047035217285\n",
      "loss:  18.30040168762207\n",
      "loss:  18.300331115722656\n",
      "loss:  18.300260543823242\n",
      "loss:  18.30019187927246\n",
      "loss:  18.30012321472168\n",
      "loss:  18.3000545501709\n",
      "loss:  18.29998207092285\n",
      "loss:  18.29991340637207\n",
      "loss:  18.29984474182129\n",
      "loss:  18.29977798461914\n",
      "loss:  18.299707412719727\n",
      "loss:  18.299636840820312\n",
      "loss:  18.29956817626953\n",
      "loss:  18.299501419067383\n",
      "loss:  18.2994327545166\n",
      "loss:  18.299362182617188\n",
      "loss:  18.299291610717773\n",
      "loss:  18.299222946166992\n",
      "loss:  18.299156188964844\n",
      "loss:  18.299087524414062\n",
      "loss:  18.29901885986328\n",
      "loss:  18.2989501953125\n",
      "loss:  18.298879623413086\n",
      "loss:  18.298810958862305\n",
      "loss:  18.298742294311523\n",
      "loss:  18.298675537109375\n",
      "loss:  18.29860496520996\n",
      "loss:  18.29853630065918\n",
      "loss:  18.29846954345703\n",
      "loss:  18.298402786254883\n",
      "loss:  18.29833221435547\n",
      "loss:  18.298263549804688\n",
      "loss:  18.298194885253906\n",
      "loss:  18.298126220703125\n",
      "loss:  18.298057556152344\n",
      "loss:  18.297990798950195\n",
      "loss:  18.297922134399414\n",
      "loss:  18.297853469848633\n",
      "loss:  18.297788619995117\n",
      "loss:  18.297719955444336\n",
      "loss:  18.297651290893555\n",
      "loss:  18.29758644104004\n",
      "loss:  18.297515869140625\n",
      "loss:  18.297449111938477\n",
      "loss:  18.297380447387695\n",
      "loss:  18.297313690185547\n",
      "loss:  18.2972469329834\n",
      "loss:  18.297178268432617\n",
      "loss:  18.297109603881836\n",
      "loss:  18.29704475402832\n",
      "loss:  18.29697608947754\n",
      "loss:  18.296907424926758\n",
      "loss:  18.296842575073242\n",
      "loss:  18.29677391052246\n",
      "loss:  18.296707153320312\n",
      "loss:  18.29663848876953\n",
      "loss:  18.296571731567383\n",
      "loss:  18.296506881713867\n",
      "loss:  18.29644012451172\n",
      "loss:  18.296371459960938\n",
      "loss:  18.296302795410156\n",
      "loss:  18.296239852905273\n",
      "loss:  18.296171188354492\n",
      "loss:  18.296104431152344\n",
      "loss:  18.296035766601562\n",
      "loss:  18.295969009399414\n",
      "loss:  18.2959041595459\n",
      "loss:  18.29583740234375\n",
      "loss:  18.2957706451416\n",
      "loss:  18.295703887939453\n",
      "loss:  18.295637130737305\n",
      "loss:  18.295570373535156\n",
      "loss:  18.295503616333008\n",
      "loss:  18.295438766479492\n",
      "loss:  18.29537010192871\n",
      "loss:  18.295305252075195\n",
      "loss:  18.29524040222168\n",
      "loss:  18.2951717376709\n",
      "loss:  18.295106887817383\n",
      "loss:  18.295042037963867\n",
      "loss:  18.29497528076172\n",
      "loss:  18.29490852355957\n",
      "loss:  18.29483985900879\n",
      "loss:  18.294775009155273\n",
      "loss:  18.294708251953125\n",
      "loss:  18.294641494750977\n",
      "loss:  18.29457664489746\n",
      "loss:  18.294511795043945\n",
      "loss:  18.29444694519043\n",
      "loss:  18.29438018798828\n",
      "loss:  18.2943172454834\n",
      "loss:  18.29425048828125\n",
      "loss:  18.2941837310791\n",
      "loss:  18.294118881225586\n",
      "loss:  18.29405403137207\n",
      "loss:  18.293989181518555\n",
      "loss:  18.293922424316406\n",
      "loss:  18.293855667114258\n",
      "loss:  18.293790817260742\n",
      "loss:  18.293725967407227\n",
      "loss:  18.29366111755371\n",
      "loss:  18.293596267700195\n",
      "loss:  18.29353141784668\n",
      "loss:  18.293466567993164\n",
      "loss:  18.29340362548828\n",
      "loss:  18.293336868286133\n",
      "loss:  18.293272018432617\n",
      "loss:  18.29320526123047\n",
      "loss:  18.29313850402832\n",
      "loss:  18.293075561523438\n",
      "loss:  18.293012619018555\n",
      "loss:  18.29294776916504\n",
      "loss:  18.292882919311523\n",
      "loss:  18.292816162109375\n",
      "loss:  18.292753219604492\n",
      "loss:  18.292688369750977\n",
      "loss:  18.29262351989746\n",
      "loss:  18.292558670043945\n",
      "loss:  18.29249382019043\n",
      "loss:  18.29243278503418\n",
      "loss:  18.29236602783203\n",
      "loss:  18.292299270629883\n",
      "loss:  18.292238235473633\n",
      "loss:  18.292173385620117\n",
      "loss:  18.2921085357666\n",
      "loss:  18.29204559326172\n",
      "loss:  18.29197883605957\n",
      "loss:  18.291915893554688\n",
      "loss:  18.29184913635254\n",
      "loss:  18.29178810119629\n",
      "loss:  18.291723251342773\n",
      "loss:  18.291658401489258\n",
      "loss:  18.291597366333008\n",
      "loss:  18.291532516479492\n",
      "loss:  18.291467666625977\n",
      "loss:  18.291404724121094\n",
      "loss:  18.29134178161621\n",
      "loss:  18.29128074645996\n",
      "loss:  18.291213989257812\n",
      "loss:  18.291147232055664\n",
      "loss:  18.291086196899414\n",
      "loss:  18.29102325439453\n",
      "loss:  18.29096031188965\n",
      "loss:  18.290895462036133\n",
      "loss:  18.290834426879883\n",
      "loss:  18.290769577026367\n",
      "loss:  18.29070472717285\n",
      "loss:  18.2906436920166\n",
      "loss:  18.290578842163086\n",
      "loss:  18.290515899658203\n",
      "loss:  18.29045295715332\n",
      "loss:  18.290390014648438\n",
      "loss:  18.290327072143555\n",
      "loss:  18.290264129638672\n",
      "loss:  18.29020118713379\n",
      "loss:  18.290138244628906\n",
      "loss:  18.29007339477539\n",
      "loss:  18.290010452270508\n",
      "loss:  18.289949417114258\n",
      "loss:  18.289884567260742\n",
      "loss:  18.289823532104492\n",
      "loss:  18.289758682250977\n",
      "loss:  18.289697647094727\n",
      "loss:  18.289636611938477\n",
      "loss:  18.28957176208496\n",
      "loss:  18.28951072692871\n",
      "loss:  18.289445877075195\n"
     ]
    }
   ],
   "source": [
    "optimize_loop(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
